# -*- coding: utf-8 -*-
"""word_embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16PxYB5LTg6PV-zI_8abUbUuoolNJEcC4

##Tokenizer example
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

texts = ['I love playing tennis',
         'I am a fan of Rafael Nadal who play tennis',
         'I also love Roger Federer']

sample_tokenizer = Tokenizer(num_words=100)
sample_tokenizer.fit_on_texts(texts)
sequence = sample_tokenizer.texts_to_sequences(texts)
print(sequence)

data = pad_sequences(sequence, maxlen=8, truncating='pre', padding='pre')
data

sample_tokenizer.texts_to_sequences(['I am raghav']) #doesn't assign any number to new unseen token

"""##Spam Detection with LSTM"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, GlobalMaxPooling1D, LSTM, SimpleRNN, Flatten
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

!wget https://lazyprogrammer.me/course_files/spam.csv

df = pd.read_csv('spam.csv', encoding='ISO-8859-1')
df.head()

df = df.loc[:,['v2','v1']].copy()
df.columns = ['data','label']
df['label'] = df['label'].map({'ham':0, 'spam':1})
df.head()

df_train, df_test = train_test_split(df, test_size=0.3, random_state=42)
df_train.shape, df_test.shape

V=10000
tokenizer = Tokenizer(num_words=V)
tokenizer.fit_on_texts(df_train['data'])
data_train = tokenizer.texts_to_sequences(df_train['data'])
data_test  = tokenizer.texts_to_sequences(df_test['data'])

len(tokenizer.word_index)

print(max(len(x) for x in data_train))

max_len = 100
data_train = pad_sequences(data_train, maxlen=max_len, truncating='pre', padding='pre')
data_test  = pad_sequences(data_test , maxlen=max_len, truncating='pre', padding='pre')
data_train.shape, data_test.shape

i = Input(shape=(max_len, ))
x = Embedding(V+1, 20)(i)
x = LSTM(15, return_sequences=True)(x)
x = GlobalMaxPooling1D()(x)
x = Dense(1, activation='sigmoid')(x)

model = Model(i,x)
model.summary()

model.compile(loss='binary_crossentropy', metrics='accuracy', optimizer='adam')
r = model.fit(data_train, df_train['label'], epochs=10, batch_size=64, validation_data=(data_test, df_test['label']))

plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()